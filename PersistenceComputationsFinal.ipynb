{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads data, computes persistence diagrams. 7/6/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to first access our training2017.zip folder and compute persistence diagrams. The code here first\n",
    "1. Stores 4 organized lists of length $k$, (we took $k$ to be 200 in our analysis) where each list is organized by classes of ECG data. The four classes are normal, atrial fibrillation, random/noise (denoted random here on out) and other. \n",
    "\n",
    "2. The function reconstruct_persist systematically computes persistence diagrams in a form that R can accept. \n",
    "\n",
    "3. Eight_Hundred_Persistence_Test takes in a folder containing the ECG graphs. It assumes you have them organized in four neat folders called Normal, AF, Random, and Other. It's called 800 because we set $k = 200.$ It then stores these persistence diagrams into csvs, although you can just use this output and plug it into reconstruct_persist to reformat the data so that R can take it in. \n",
    "\n",
    "4. TXT_to_ARRAY assumes also takes in a folder and it assumes you have data written in .txt form in four neat folders, again called Normal, AF, Random, and Other (right now we only have 3, but that can be added later). The reason why it takes txt files is because that is the output of another function, which cleans up the data and removes noise.\n",
    "\n",
    "5. Persistence_Lists takes in clean data and returns a list where each element is the persistence diagram of every element of the clean data. \n",
    "\n",
    "6. iterative_reconstruct_persist iterates over the previously written function reconstruct_persist by reformating the persistence diagrams of the clean data in a way that R will accept.\n",
    "\n",
    "7. saveAllPersistenceForR takes in the output of iterative_reconstruct_persist and simply stores all of the persistence diagrams into CSV's. This is so that we can then load these CSVs into R and go right ahead in computing clusters. Be sure you are aware of what your current working directory is if you plan on running this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import cython as cy\n",
    "\n",
    "import dionysus as d\n",
    "from dionysus import *\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from ripser import ripser, plot_dgms\n",
    "from ripser import Rips\n",
    "\n",
    "import wfdb\n",
    "\n",
    "r = Rips(maxdim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = pd.read_csv(\"REFERENCE-v3.csv\", names = [\"ECG run\", \"Type\"])\n",
    "\n",
    "N = [] #Store your data.\n",
    "A = []\n",
    "R = []\n",
    "O = []\n",
    "l = len(reference[\"ECG run\"])\n",
    "\n",
    "def kSublists(k): #Computes 4 organized lists of length k. \n",
    "    for i in range(l):    \n",
    "        if reference[\"Type\"][i] == 'N':\n",
    "            if len(N) < k:\n",
    "                N.append(reference[\"ECG run\"][i])\n",
    "        elif reference[\"Type\"][i] == 'A':\n",
    "            if len(A) < k:\n",
    "                A.append(reference[\"ECG run\"][i])\n",
    "        elif reference[\"Type\"][i] == '~':\n",
    "            if len(R) < k:\n",
    "                R.append(reference[\"ECG run\"][i])\n",
    "        elif reference[\"Type\"][i] == \"O\":\n",
    "            if len(O) < k:\n",
    "                O.append(reference[\"ECG run\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_persist(persistence): #Takes in an array of length i containing nx2 arrays, where n is the number\n",
    "    total_dim = len(persistence)      #of points in the i-th dimension. The first column of the nx2 arrays contain\n",
    "    length = 0                        #birth points, while the second column contains corresponding death points.\n",
    "    k = 0 #keeps track of columns, see the for loop below\n",
    "    for i in range(total_dim):\n",
    "        length += len(persistence[i])\n",
    "    persistence_Matrix = np.zeros(length*3).reshape(length, 3) #populate empty matrix\n",
    "    \n",
    "    for i in range(total_dim):\n",
    "        k = 0\n",
    "        if i != 0: #This if statement guarantees that the columns are stored correctly.\n",
    "            for p in range(i):\n",
    "                k += len(persistence[p])\n",
    "        for j in range(len(persistence[i])):\n",
    "            persistence_Matrix[j+k,0] = i #stores dimension in the first column\n",
    "            persistence_Matrix[j+k,1] = persistence[i][j][0] #stores x coordinate, or birth\n",
    "            persistence_Matrix[j+k,2] = persistence[i][j][1] #stores y coordinate, or death.\n",
    "    return persistence_Matrix\n",
    "\n",
    "def Eight_Hundred_Persistence_Test(folder): #Computes 800 persistence diagrams, given a directory of the location of \n",
    "    for i in range(800):                    #the data points. \n",
    "        if 0 <= i < 200:\n",
    "            signals, fields = wfdb.rdsamp(folder + '/200 normal/' + N[i])\n",
    "            diagram = r.fit_transform(signals)\n",
    "            df = pd.DataFrame(reconstruct_persist(diagram))\n",
    "            df.to_csv(\"Persistence Matrices/200 Normal Ps/\" + N[i] + \"PersMatrix.csv\")\n",
    "        if 200 <= i < 400:\n",
    "            signals, fields = wfdb.rdsamp(folder + '/200 af/' + AF[i-200])\n",
    "            diagram = r.fit_transform(signals)\n",
    "            df = pd.DataFrame(reconstruct_persist(diagram))\n",
    "            df.to_csv(\"Persistence Matrices/200 AF Ps/\" + AF[i-200] + \"PersMatrix.csv\")            \n",
    "        if 400 <= i < 600:\n",
    "            signals, fields = wfdb.rdsamp(folder + '/200 random/' + O[i-400])\n",
    "            diagram = r.fit_transform(signals)\n",
    "            df = pd.DataFrame(reconstruct_persiste(diagram))\n",
    "            df.to_csv(\"Persistence Matrices/200 Random Ps/\" + O[i-400] + \"PersMatrix.csv\")            \n",
    "        if 600 <= i < 800:\n",
    "            signals, fields = wfdb.rdsamp(folder + '/200 other/' + R[i-600])\n",
    "            diagram = r.fit_transform(signals)\n",
    "            df = pd.DataFrame(reconstruct_persiste(diagram))\n",
    "            df.to_csv(\"Persistence Matrices/200 Normal/\" + R[i-600] + \"PersMatrix.csv\")        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_clean = [] #Lists to store our clean data.\n",
    "AF_clean = []\n",
    "R_clean = []\n",
    "O_clean = []\n",
    "\n",
    "N_clean_labels = [] #Lists to keep track of the labels of our clean data.\n",
    "AF_clean_labels = []\n",
    "R_clean_labels = []\n",
    "O_clean_labels = []\n",
    "\n",
    "def TXT_to_ARRAY(direct): #Specify your GITHUB directory, i.e., wherever the AF, Normal, and Random folders are stored\n",
    "    for file in os.listdir(direct + \"/Normal\"):\n",
    "        filename = os.fsdecode(file)\n",
    "        if len(N_clean) < 200:\n",
    "            if filename.endswith(\".txt\"): #Hopefully you don't have any other .txt files in there...\n",
    "                lines = np.array(pd.read_csv(\"TDA-MSRI-2018/Normal/\" + filename))\n",
    "                if np.shape(lines) != (): #We don't want empty empty vectors.\n",
    "                    lines.reshape(len(lines), 1) #Reformats into a column vector.\n",
    "                    if len(lines) > 1:    #Required double check so that we don't get single elements. \n",
    "                        N_clean.append(lines) \n",
    "                        N_clean_labels.append(filename)\n",
    "            \n",
    "    for file in os.listdir(direct + \"/AF\"):\n",
    "        filename = os.fsdecode(file)\n",
    "        if len(AF_clean) < 200:\n",
    "            if filename.endswith(\".txt\"): \n",
    "                lines = np.array(pd.read_csv(\"TDA-MSRI-2018/AF/\" + filename))\n",
    "                if np.shape(lines) != ():\n",
    "                    lines.reshape(len(lines), 1) \n",
    "                    if len(lines) > 1:\n",
    "                        AF_clean.append(lines)\n",
    "                        AF_clean_labels.append(filename)\n",
    "            \n",
    "    for file in os.listdir(direct + \"/Random\"):\n",
    "        filename = os.fsdecode(file)\n",
    "        if len(R_clean) < 200:\n",
    "            if filename.endswith(\".txt\"): \n",
    "                lines = np.array(pd.read_csv(\"TDA-MSRI-2018/Random/\" + filename))\n",
    "                if np.shape(lines) != ():    \n",
    "                    lines.reshape(len(lines), 1) \n",
    "                    if len(lines) > 1:    \n",
    "                        R_clean.append(lines) \n",
    "                        R_clean_labels.append(filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_All = N_clean + AF_clean + R_clean\n",
    "\n",
    "def Persistence_Lists(All_clean_vectors): #Takes in the clean vectors (noise removed) and outputs a list where each\n",
    "    List_of_Persistence = []              #element is a clean vector's persistence diagram.\n",
    "    for i in range(len(All_clean_vectors)):\n",
    "        diagram = r.fit_transform(All_clean_vectors[i])\n",
    "        List_of_Persistence.append(diagram)\n",
    "    return List_of_Persistence\n",
    "\n",
    "List_of_R_Persistence = []\n",
    "def iterative_reconstruct_persist(thepersistence): #This function reformats the persistence diagrams into a form that \n",
    "    for i in range(len(All_clean_vectors)):                           #R can accept. This simply calls reconstruct_persist.\n",
    "        List_of_R_Persistence.append(reconstruct_persist(thepersistence[i]))\n",
    "        \n",
    "Labels = R_clean_labels + N_clean_labels + AF_clean_labels\n",
    "def saveAllPersistenceForR(final_list, theLabels): #final_list is the list of persistence diagrams written in a form \n",
    "    for i in range(600):                           #that R can accept. You plug in the output of iterative_reconstruct_persist.\n",
    "        K = pd.DataFrame(final_list[i])\n",
    "        K.to_csv(theLabels[i] + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
